#!/bin/bash

#SBATCH --job-name=tokenize      # Job name
#SBATCH --ntasks=1               # total number of tasks across all nodes
#SBATCH --cpus-per-task=32       # cpu-cores per task (>1 if multi-threaded tasks)
#SBATCH --hint=multithread       # we get physical cores not logical
#SBATCH --time=24:00:00               # Time limit hrs:min:sec
#SBATCH --output=/scratch/lnishimw/logs/robust-embeddings/laser/tokenize/tokenize_%j.log # Standard output and error log
#SBATCH --array=0-4

set -e 

echo "### Running $SLURM_JOB_NAME $SLURM_ARRAY_TASK_ID ###"

source $HOME/.bashrc
source $HOME/.bash_profile
source activate laser_env

# to be able to use fairseq files
FAIRSEQ_PATH=$SCRATCH/fairseq/
export PYTHONPATH=$PYTHONPATH:$FAIRSEQ_PATH

MODEL_NAME[0]=laser
TOKENIZER[0]=spm

MODEL_NAME[1]=roberta-student
TOKENIZER[1]=roberta

MODEL_NAME[2]=roberta-student-init
TOKENIZER[2]=roberta

MODEL_NAME[3]=character-roberta-student
TOKENIZER[3]=char

MODEL_NAME[4]=character-roberta-student-init
TOKENIZER[4]=char

EXPERIMENT_DIR=$EXPERIMENTS/robust-embeddings/laser/experiment_025_lrec

# CORPUS=lexnorm2015
# CORPUS_PARTS="train test"
# FILES="uncased.raw.src uncased.raw.ref"

CORPUS=multilexnorm2021
CORPUS_PARTS="train dev test"
FILES="en.src en.ref"

# CORPUS=rocsmt
# CORPUS_PARTS="test"
# FILES="src.raw-manseg.en src.norm-manseg.en"

for CORPUS_PART in $CORPUS_PARTS
do
    INPUT_DIR=$DATASETS/$CORPUS/cleaned/$CORPUS_PART
    OUTPUT_TOK_DIR=$EXPERIMENT_DIR/tok/${MODEL_NAME[$SLURM_ARRAY_TASK_ID]}/$CORPUS/$CORPUS_PART
    mkdir -p $OUTPUT_TOK_DIR

    for INPUT_FILE_NAME in $FILES
    do
        INPUT_FILE_NAME=cleaned.$INPUT_FILE_NAME.$CORPUS_PART
        INPUT_FILE=$INPUT_DIR/$INPUT_FILE_NAME
        OUTPUT_TOK_FILE=$OUTPUT_TOK_DIR/$INPUT_FILE_NAME
        
        echo "Tokenizing input file $INPUT_FILE_NAME..."

        python $HOME/data-preparation/src/tokenizer.py \
            -i $INPUT_FILE \
            -o $OUTPUT_TOK_FILE \
            -t ${TOKENIZER[$SLURM_ARRAY_TASK_ID]}
    done
done
echo "Done..."

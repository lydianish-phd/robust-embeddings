#!/bin/bash

#SBATCH --job-name=preprocess      # Job name
#SBATCH --ntasks=1               # total number of tasks across all nodes
#SBATCH --cpus-per-task=64       # cpu-cores per task (>1 if multi-threaded tasks)
#SBATCH --hint=multithread       # we get physical cores not logical
#SBATCH --time=24:00:00               # Time limit hrs:min:sec
#SBATCH --output=/scratch/lnishimw/logs/robust-embeddings/laser/preprocess/preprocess_%j.log # Standard output and error log
#SBATCH --array=0-1

echo "### Running $SLURM_JOB_NAME ###"

source $HOME/.bashrc
source $HOME/.bash_profile
source activate laser_env

# to be able to use fairseq files
FAIRSEQ_PATH=$SCRATCH/fairseq
export PYTHONPATH=$PYTHONPATH:$FAIRSEQ_PATH

STUDENT_VOCAB=$MODELS/checkpoints/roberta-base/roberta.cvocab
TEACHER_VOCAB=$LASER/models/laser2.cvocab
TEACHER_LANG=laserstd
STUDENT_LANG[0]=robertastd
STUDENT_LANG[1]=robertaugc

DATA_DIR=$DATASETS/oscar/mini/4Mclean
TOK_DIR=$DATA_DIR/tok/${STUDENT_LANG[$SLURM_ARRAY_TASK_ID]}-$TEACHER_LANG
BIN_DIR=$DATA_DIR/bin/${STUDENT_LANG[$SLURM_ARRAY_TASK_ID]}-$TEACHER_LANG

echo "Creating dictionary and binarizing data..."

fairseq-preprocess --source-lang ${STUDENT_LANG[$SLURM_ARRAY_TASK_ID]} --target-lang $TEACHER_LANG \
    --trainpref $TOK_DIR/train \
    --destdir $BIN_DIR \
    --tgtdict $TEACHER_VOCAB \
    --srcdict $STUDENT_VOCAB \
    --dataset-impl cached \
    --workers 16 \

echo "Done..."

#!/bin/bash

#SBATCH --job-name=tokenize      # Job name
#SBATCH --ntasks=1               # total number of tasks across all nodes
#SBATCH --cpus-per-task=32       # cpu-cores per task (>1 if multi-threaded tasks)
#SBATCH --hint=multithread       # we get physical cores not logical
#SBATCH --time=24:00:00               # Time limit hrs:min:sec
#SBATCH --output=/scratch/lnishimw/logs/robust-embeddings/laser/%x/%x_%j.log # Standard output and error log
#SBATCH --array=0-4

set -e 

echo "### Running $SLURM_JOB_NAME $SLURM_ARRAY_TASK_ID ###"

source $HOME/.bashrc
source $HOME/.bash_profile
source activate laser_env

MODEL_NAME[0]=laser
TOKENIZER[0]=spm

MODEL_NAME[1]=roberta-student
TOKENIZER[1]=roberta

MODEL_NAME[2]=roberta-student-init
TOKENIZER[2]=roberta

MODEL_NAME[3]=character-roberta-student
TOKENIZER[3]=char

MODEL_NAME[4]=character-roberta-student-init
TOKENIZER[4]=char

EXPERIMENT_DIR=$EXPERIMENTS/robust-embeddings/laser/experiment_025_lrec

CORPUS=flores200

CORPUS_PARTS="dev devtest"
FILES="eng_Latn eng_abr1_Latn eng_abr2_Latn \
    eng_abr3_Latn eng_case_Latn eng_cont_Latn \
    eng_dysl_Latn eng_fing_Latn eng_homo_Latn \
    eng_leet_Latn eng_slng_Latn eng_spac_Latn \
    eng_spel_Latn eng_week_Latn eng_Latn_ugc"

# CORPUS_PARTS="dev"
# FILES="eng_Latn eng_Latn_ugc"

# for SEED in "110"
for SEED in {100..109}
do
    for CORPUS_PART in $CORPUS_PARTS
    do
        INPUT_DIR=$DATASETS/$CORPUS/cleaned/ugc/$SEED/$CORPUS_PART
        OUTPUT_TOK_DIR=$EXPERIMENT_DIR/tok/${MODEL_NAME[$SLURM_ARRAY_TASK_ID]}/$CORPUS/$SEED/$CORPUS_PART
        mkdir -p $OUTPUT_TOK_DIR

        for INPUT_FILE_NAME in $FILES
        do
            INPUT_FILE_NAME=cleaned.$INPUT_FILE_NAME.$CORPUS_PART
            INPUT_FILE=$INPUT_DIR/$INPUT_FILE_NAME
            OUTPUT_TOK_FILE=$OUTPUT_TOK_DIR/$INPUT_FILE_NAME
            
            echo "Tokenizing input file $INPUT_FILE_NAME..."

            python $HOME/data-preparation/src/tokenizer.py \
                -i $INPUT_FILE \
                -o $OUTPUT_TOK_FILE \
                -t ${TOKENIZER[$SLURM_ARRAY_TASK_ID]}
        done
    done
done

for CORPUS_PART in $CORPUS_PARTS
do
    INPUT_DIR=$DATASETS/$CORPUS/cleaned/ugc/100/${CORPUS_PART}_augmented
    OUTPUT_TOK_DIR=$EXPERIMENT_DIR/tok/${MODEL_NAME[$SLURM_ARRAY_TASK_ID]}/$CORPUS/100/${CORPUS_PART}_augmented
    mkdir -p $OUTPUT_TOK_DIR

    FILES="eng_Latn_augmented.$CORPUS_PART eng_Latn_errtype.$CORPUS_PART.json"
    for INPUT_FILE_NAME in $FILES
    do
        INPUT_FILE_NAME=cleaned.$INPUT_FILE_NAME
        INPUT_FILE=$INPUT_DIR/$INPUT_FILE_NAME
        OUTPUT_TOK_FILE=$OUTPUT_TOK_DIR/$INPUT_FILE_NAME
        
        echo "Tokenizing input file $INPUT_FILE_NAME..."

        python $HOME/data-preparation/src/tokenizer.py \
            -i $INPUT_FILE \
            -o $OUTPUT_TOK_FILE \
            -t ${TOKENIZER[$SLURM_ARRAY_TASK_ID]}
    done

    for SEED in {101..109}
    do
        OTHER_OUTPUT_TOK_DIR=$EXPERIMENT_DIR/tok/${MODEL_NAME[$SLURM_ARRAY_TASK_ID]}/$CORPUS/$SEED/${CORPUS_PART}_augmented
        mkdir -p $OTHER_OUTPUT_TOK_DIR
        cp $OUTPUT_TOK_DIR/* $OTHER_OUTPUT_TOK_DIR
    done 
done


echo "Done..."

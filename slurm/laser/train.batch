#!/bin/bash

#SBATCH --job-name=train      # Job name
#SBATCH --nodes=1                # node count
#SBATCH --ntasks=1               # total number of tasks across all nodes
#SBATCH --partition=almanach          # Name of the partition
#SBATCH --account=almanach        # GPU nodes are only available in gpu partition
#SBATCH --gres=gpu:rtx8000:1        # GPU nodes are only available in gpu partition
#SBATCH --hint=nomultithread       # we get physical cores not logical
#SBATCH --time=48:00:00                # Time limit hrs:min:sec
#SBATCH --output=/scratch/lnishimw/logs/robust-embeddings/laser/%x/%x_%j.log # Standard output and error log

echo "### Running $SLURM_JOB_NAME ###"

module purge
module load cuda/10.2

source $HOME/.bashrc 
source $HOME/.bash_profile
source $HOME/.bash_python_exports
source activate laser_env

set -x

TEACHER_MODEL=$LASER/models/laser2.pt
STUDENT_MODEL=$MODELS/checkpoints/roberta-base/pytorch_model.bin
EXPERIMENT_DIR=$EXPERIMENTS/robust-embeddings/laser/experiment_024
OUTPUT_DIR=$EXPERIMENT_DIR/models
TENSORBOARD_DIR=$EXPERIMENT_DIR/tensorboard
CONFIG_PATH=$EXPERIMENT_DIR/train-config.json

echo "Training student model $SLURM_ARRAY_TASK_ID..."

fairseq-train \
    $CONFIG_PATH \
    --user-dir $FAIRSEQ_PATH/examples/nllb/laser_distillation \
    --log-interval 100  \
    --log-format simple \
    --task laser_distillation \
    --arch laser_transformer \
    --sentemb-criterion maxpool \
    --prepend-bos  \
    --criterion encoder_similarity \
    --mse \
    --save-dir $OUTPUT_DIR \
    --optimizer adam  \
    --adam-betas '(0.9,0.98)'  \
    --adam-eps 1e-6 \
    --lr 0.0001 \
    --warmup-updates 1000 \
    --clip-norm 5 \
    --update-freq 1 \
    --dropout 0.3 \
    --max-tokens 4000 \
    --max-epoch 50 \
    --student-bpe-symbol roberta \
    --left-pad-source False \
    --left-pad-target True \
    --encoder-embed-dim 768 \
    --encoder-layers 12 \
    --encoder-ffn-embed-dim 3072 \
    --encoder-attention-heads 12 \
    --decoder-embed-dim 1 \
    --decoder-layers 1 \
    --decoder-ffn-embed-dim 1 \
    --decoder-attention-heads 1 \
    --decoder-lang-embed-dim 1 \
    --ddp-backend=no_c10d \
    --save-interval-updates 30000 \
    --disable-validation \
    --teacher-checkpoint-path $TEACHER_MODEL \
    --lambda-self 0 \
    --lambda-mask 0 \
    --lambda-distil 1.0 \
    --student-teacher-config "distil:robertaugc-laserstd,distil:robertastd-laserstd" \
    --tensorboard-logdir $TENSORBOARD_DIR \

echo "Done..."

#     --student-checkpoint-path $STUDENT_MODEL \

#!/bin/bash

#SBATCH --job-name=dictionary      # Job name
#SBATCH --ntasks=1               # total number of tasks across all nodes
#SBATCH --cpus-per-task=64       # cpu-cores per task (>1 if multi-threaded tasks)
#SBATCH --hint=multithread       # we get physical cores not logical
#SBATCH --time=24:00:00               # Time limit hrs:min:sec
#SBATCH --output=/scratch/lnishimw/logs/robust-embeddings/laser/%x/%x_%j.log # Standard output and error log

echo "### Running $SLURM_JOB_NAME ###"

source $HOME/.bashrc
source $HOME/.bash_profile
source activate laser_env

STUDENT_LANG[0]=charbertstd
STUDENT_LANG[1]=charbertugc

DATA_DIR=/home/lnishimw/scratch/datasets/oscar/mini/4M
TOK_DIR=$DATA_DIR/tok/${STUDENT_LANG[1]}-${STUDENT_LANG[0]}
BIN_DIR=$DATA_DIR/bin/${STUDENT_LANG[1]}-${STUDENT_LANG[0]}

echo "Creating joint dictionary from pretokenized data..."

fairseq-preprocess --source-lang ${STUDENT_LANG[1]} --target-lang ${STUDENT_LANG[0]} \
    --trainpref $TOK_DIR/train \
    --destdir $BIN_DIR \
    --dataset-impl cached \
    --workers 16 \
    --dict-only \
    --joined-dictionary \

echo "Done..."